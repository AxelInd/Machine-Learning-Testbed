{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54af4a7",
   "metadata": {},
   "source": [
    "Data Source: https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification\n",
    "\n",
    "Process Source: https://www.youtube.com/watch?v=szyGiObZymo&list=PL-wATfeyAMNrtbkCNsLcpoAyBBRJZVlnf&index=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9d01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import math\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a2af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@param dataset_path: relative location of the dataset.\n",
    "@param json_path:\n",
    "@param n_mfcc: number of mfcc variables to receive\n",
    "@param hop_length: how far along to move the window\n",
    "@param num_segments: how many parts to break each file into\n",
    "\"\"\"\n",
    "def create_mfcc_data(dataset_path, n_mfcc=13, n_fft=2048, hop_length=512, semanticLabelFolderLocation=-2,\n",
    "              num_segments=5, sample_rate=20000,allLabels={\"speech\":1,\"rap\":2,\"singing\":3},duration = 3):\n",
    "    #dictionary to store data\n",
    "    # - mapping: mapp labels to values\n",
    "    # - labels: target\n",
    "    data={\n",
    "        \"mapping\":[],\n",
    "        \"mfcc\":[],\n",
    "        \"labels\":[]\n",
    "    }\n",
    "    #the length of each sample, we know this for this dataset. Could be calculated dynamically\n",
    "    \n",
    "    samples_per_track = sample_rate * duration\n",
    "    #how many data points we expect to appear in each segment we break our track into\n",
    "    num_samples_per_segment = int(samples_per_track/num_segments)\n",
    "    expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segment/ hop_length)\n",
    "    \n",
    "    print (\"num_samples_per_segment\",num_samples_per_segment)\n",
    "    #loop through all the genres\n",
    "    #dirpath: folder we are currently in\n",
    "    #dirnames: all the names of the subfolders\n",
    "    #filenames: all the filenames\n",
    "    #i: the count. It MUST be included\n",
    "    #os.walk returns a generator, that creates a tuple of values: \n",
    "    #(current_path, directories in current_path, files in current_path).\n",
    "    # - each iteration is a different genre\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        #ensure we're not yet at the dataset level\n",
    "        if dirpath is not dataset_path:\n",
    "            #save the semantic label (the mappings etc)\n",
    "            #dirpath_components: the individual folder names that make up the full path\n",
    "            dirpath_components = dirpath.split(\"\\\\\") #genre/blues => [\"genre\",\"blues\"]\n",
    "            semantic_label = dirpath_components[semanticLabelFolderLocation]\n",
    "            #use the parent directory of a sound file as its label\n",
    "            data[\"mapping\"].append(semantic_label)\n",
    "            for f in filenames:\n",
    "                # load audio file : the path of the file is just its directory path plus its name\n",
    "                file_path = os.path.join(dirpath,f)\n",
    "                signal, sr = librosa.load(file_path,sr=sample_rate)\n",
    "                #process segments to extract MFCC and store data\n",
    "                for s in range (0, num_segments):\n",
    "                    start_sample = num_samples_per_segment * s\n",
    "                    finish_sample = start_sample + num_samples_per_segment\n",
    "                    #the mfcc for data points between start_sample and finish_sample\n",
    "                    mfcc = librosa.feature.mfcc(signal[start_sample:finish_sample],sr=sr,n_fft=n_fft,n_mfcc=n_mfcc, hop_length = hop_length)\n",
    "                    mfcc=mfcc.T\n",
    "                    #store mfcc for segement if it has the expected length\n",
    "                    if len(mfcc)==expected_num_mfcc_vectors_per_segment:\n",
    "                        data[\"mfcc\"].append(mfcc.tolist())\n",
    "                        # first iteration was the dataset path\n",
    "                        correctLabel = allLabels[semantic_label]\n",
    "                        data[\"labels\"].append(correctLabel)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f834a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc_data(data,json_path):\n",
    "    #save what we have created as a jason file\n",
    "    with open(json_path,\"w\") as fp:\n",
    "        json.dump(data,fp,indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab19d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples_per_segment 48000\n",
      "Run Complete. Time taken: 10.474484200000001\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH=\"./data\"\n",
    "JSON_PATH=\"data.json\"\n",
    "SAMPLE_RATE = 16000\n",
    "N_MFCC = 13\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH=512\n",
    "NUM_SEGMENTS = 1\n",
    "ALLLABELS={\"speech\":1,\"rap\":2,\"singing\":3}\n",
    "DURATION = 3\n",
    "SEMANTICLABELFOLDERLOCATION=-2\n",
    "\n",
    "startTime = time.perf_counter()\n",
    "data=create_mfcc_data(dataset_path=DATASET_PATH, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH, \n",
    "                      semanticLabelFolderLocation=SEMANTICLABELFOLDERLOCATION, num_segments=NUM_SEGMENTS, \n",
    "                      sample_rate=SAMPLE_RATE,allLabels=ALLLABELS,duration = DURATION)\n",
    "save_mfcc_data(data,JSON_PATH)\n",
    "endTime = time.perf_counter()\n",
    "\n",
    "print (\"Run Complete. Time taken:\",endTime-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614269ae",
   "metadata": {},
   "source": [
    "The Network Itself\n",
    "Source: https://www.youtube.com/watch?v=_xcFAiufwd0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d41884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#load data\n",
    "\n",
    "# split data into training and test\n",
    "\n",
    "def load_data(dataset_path):\n",
    "    with open(dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "        #convert lists into numpy arrays\n",
    "        # x\n",
    "        inputs=np.array(data[\"mfcc\"])\n",
    "        targets=np.array(data[\"labels\"])\n",
    "        return inputs,targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14027685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape (1440, 94, 13)\n",
      "Frequency of unique values of the targets:\n",
      "[[  1   2   3]\n",
      " [480 480 480]]\n"
     ]
    }
   ],
   "source": [
    "inputs,targets = load_data(\"data.json\")\n",
    "#inputs.shape[0] num samples\n",
    "#inputs.shape[1] num time readings\n",
    "#inputs.shape[2] num values per time interval\n",
    "print (\"input shape\", inputs.shape)\n",
    "\n",
    "#normalise the data. @TODO not sure if this is necessary\n",
    "mean = np.mean(inputs)\n",
    "std = np.std(inputs)\n",
    "inputs=(inputs-mean)/std\n",
    "\n",
    "unique_elements, counts_elements = np.unique(targets, return_counts=True)\n",
    "print(\"Frequency of unique values of the targets:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a1f87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (1296, 94, 13)\n",
      "x_test (144, 94, 13)\n",
      "y_train (1296,)\n",
      "y_test (144,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(inputs,targets,test_size=0.1)\n",
    "\n",
    "print (\"x_train\",x_train.shape)\n",
    "print (\"x_test\",x_test.shape)\n",
    "print (\"y_train\",y_train.shape)\n",
    "print (\"y_test\",y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99c12ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1222)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               626176    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 774,602\n",
      "Trainable params: 774,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "#we will use only a traditional dense mlp for this first test, but CNNs would be far more accurate\n",
    "#takes multidimensional input and treats it as a vector\n",
    "#mfcc data is mfcc values taken at itervals. Second dimension is values for that interval.\n",
    "flattenLayer = keras.layers.Flatten(input_shape=(inputs.shape[1],inputs.shape[2]))\n",
    "#relu: activation function outputs 0 if net input is less than 0. else outputs h.\n",
    "#relu is very very effective for training (much faster convergence)\n",
    "# reduced probability of vanishing gradient: derivative of sigmoid can't be higher than 0,25 so it shrinks and becomes tiny\n",
    "# relu does not have this problem. So relu allows us to have much deeper networks\n",
    "#3 hidden layers\n",
    "denseLayer1 = keras.layers.Dense(512,activation=\"relu\")\n",
    "denseLayer2 = keras.layers.Dense(256,activation=\"relu\")\n",
    "denseLayer3 = keras.layers.Dense(64,activation=\"relu\")\n",
    "\n",
    "#we have 10 categories\n",
    "#softmax is an activation function that normalised the output (so total is 1)\n",
    "outputLayer = keras.layers.Dense(10,activation=\"softmax\")\n",
    "model=keras.Sequential([flattenLayer,denseLayer1,denseLayer2,denseLayer3,outputLayer])\n",
    "\n",
    "#Adam is a very very effecting sgd variant for deep learning\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#put all the components together\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"]\n",
    "              )\n",
    "#describe our network\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dbcd21",
   "metadata": {},
   "source": [
    "\n",
    "Dealing with overfitting\n",
    "\n",
    "Source: https://www.youtube.com/watch?v=Gf5DO6br0ts&list=PL-wATfeyAMNrtbkCNsLcpoAyBBRJZVlnf&index=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776ea594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    fig,axs = plt.subplots(2)\n",
    "    axs[0].plot(history.history[\"accuracy\"],label=\"train accuracy\")\n",
    "    axs[0].plot(history.history[\"val_accuracy\"],label=\"test accuracy\")\n",
    "    axs[0].set_ylabel(\"accuracy\")\n",
    "    #loc sets location\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"accuracy eval\")\n",
    "    \n",
    "    axs[1].plot(history.history[\"loss\"],label=\"train error\")\n",
    "    axs[1].plot(history.history[\"val_loss\"],label=\"test error\")\n",
    "    axs[1].set_ylabel(\"error\")\n",
    "    #loc sets location\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"error eval\")\n",
    "    axs[1].set_xlabel(\"error\")\n",
    "    axs[1].set_xlabel(\"epoch\")\n",
    "    \n",
    "    #just keeps the images from overlapping\n",
    "    fig.tight_layout() \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b86112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the network\n",
    "#batch-size usually 16-128 samples, quick, memory light and fairly accurate\n",
    "#note the very higha ccuract of the test set and low accuracy of the training set (~98% vs ~58%)\n",
    "# we are overfitting\n",
    "#mode.fit returns \"A History object. Its History.history attribute is a record of training loss values and metrics\n",
    "#values at successive epochs, as well as validation loss values and validation metrics values (if applicable)\"\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a13018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice that accuracy stops increasing for the test set pretty soon\n",
    "# a sure sign of overfitting\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c417b",
   "metadata": {},
   "source": [
    "<h2>Solutions to overfitting</h2>\n",
    "<ul>\n",
    "    <li>Simplify your architecture</li>\n",
    "    <li> Audio data augmentation: increase number of training samples. Can artificially build training samples (e.g. with pitch shifting, background noise, and time stretching) -- only augment the TRAINING SET</li>\n",
    "    <li> Early stopping: stop on conditions other than total num epochs</li>\n",
    "    <li> Dropout: randomly (stochastically) drop neurons whilst training. 10% - 50% chance</li>\n",
    "    <li> Regularisation: penalty to the error function. Punishes large weights. (L1 and L2 are commong types). L1: Appends absolute error to the standard error function. L2: squares each weights contribution to the standard error. (good for complex learning, but outliers are very bad)</li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use only a traditional dense mlp for this first test, but CNNs would be far more accurate\n",
    "#takes multidimensional input and treats it as a vector\n",
    "#mfcc data is mfcc values taken at itervals. Second dimension is values for that interval.\n",
    "flattenLayer = keras.layers.Flatten(input_shape=(inputs.shape[1],inputs.shape[2]))\n",
    "#relu: activation function outputs 0 if net input is less than 0. else outputs h.\n",
    "#relu is very very effective for training (much faster convergence)\n",
    "# reduced probability of vanishing gradient: derivative of sigmoid can't be higher than 0,25 so it shrinks and becomes tiny\n",
    "# relu does not have this problem. So relu allows us to have much deeper networks\n",
    "#3 hidden layers\n",
    "#IMPORTANT:: we are also adding REGULARIZER as well as dropout\n",
    "denseLayer1 = keras.layers.Dense(512,activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.0001))\n",
    "#param is dropout probability\n",
    "dropoutLayer1 = keras.layers.Dropout(0.3)\n",
    "denseLayer2 = keras.layers.Dense(256,activation=\"relu\",kernel_regularizer=keras.regularizers.l2(0.0001))\n",
    "dropoutLayer2 = keras.layers.Dropout(0.3)\n",
    "denseLayer3 = keras.layers.Dense(64,activation=\"relu\",kernel_regularizer=keras.regularizers.l2(0.0001))\n",
    "dropoutLayer3 = keras.layers.Dropout(0.3)\n",
    "#we have 10 categories\n",
    "#softmax is an activation function that normalised the output (so total is 1)\n",
    "outputLayer = keras.layers.Dense(10,activation=\"softmax\")\n",
    "model_with_dropout=keras.Sequential([flattenLayer,denseLayer1,dropoutLayer1,denseLayer2,dropoutLayer2,\n",
    "                                     denseLayer3,dropoutLayer3,outputLayer])\n",
    "\n",
    "#Adam is a very very effecting sgd variant for deep learning\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#put all the components together\n",
    "model_with_dropout.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"]\n",
    "              )\n",
    "#describe our network\n",
    "model_with_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c6829",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_with_dropout = model_with_dropout.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=75, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_with_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ff521",
   "metadata": {},
   "source": [
    "CNNS:\n",
    "fewer parameters than desnse layers, perform better\n",
    "underlying assumption: image and audio data are somehow structured\n",
    "Relies on two special layers: convolution, pooling\n",
    "\n",
    "convolution: applies the kernel to an image\n",
    "\n",
    "shape of kernel determines what features it can extract\n",
    "\n",
    "Architectural decisions CNN: grid size, stride, num kernels, depth (dimensions of the input)\n",
    "\n",
    "CNN outputs as many 2ds arrays as the number of kernels for each conv layer\n",
    "\n",
    "pooling: downsample an image. No parameters. Max pooling just takes the max over its grid and outputs that.\n",
    "\n",
    "Flatten at the end for the final fully connected layer(s)\n",
    "\n",
    "For audio data: we can just use the image of a spectrogram as input (treat audio as an image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87dc896",
   "metadata": {},
   "source": [
    "MFCCs for CNNs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458f982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf220ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ed550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46166c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a04cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12a8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6ccb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
