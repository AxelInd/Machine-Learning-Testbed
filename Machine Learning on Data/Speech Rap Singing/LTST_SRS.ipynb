{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88414c5",
   "metadata": {},
   "source": [
    "<h1> Speech Rap Signing Classification - LSTM</h1>\n",
    "\n",
    "Source: https://www.youtube.com/watch?v=4nXI0h2sq2I&list=PL-wATfeyAMNrtbkCNsLcpoAyBBRJZVlnf&index=19\n",
    "\n",
    "Data source: see my SRS_classifier.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3579f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c80400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@param dataset_path: relative location of the dataset.\n",
    "@param json_path:\n",
    "@param n_mfcc: number of mfcc variables to receive\n",
    "@param hop_length: how far along to move the window\n",
    "@param num_segments: how many parts to break each file into\n",
    "\"\"\"\n",
    "def create_mfcc_data(dataset_path, n_mfcc=13, n_fft=2048, hop_length=512, semanticLabelFolderLocation=-2,\n",
    "              num_segments=5, sample_rate=20000,allLabels={\"speech\":1,\"rap\":2,\"singing\":3},duration = 3):\n",
    "    #dictionary to store data\n",
    "    # - mapping: mapp labels to values\n",
    "    # - labels: target\n",
    "    data={\n",
    "        \"mapping\":[],\n",
    "        \"mfcc\":[],\n",
    "        \"labels\":[]\n",
    "    }\n",
    "    #the length of each sample, we know this for this dataset. Could be calculated dynamically\n",
    "    \n",
    "    samples_per_track = sample_rate * duration\n",
    "    #how many data points we expect to appear in each segment we break our track into\n",
    "    num_samples_per_segment = int(samples_per_track/num_segments)\n",
    "    expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segment/ hop_length)\n",
    "    \n",
    "    print (\"num_samples_per_segment\",num_samples_per_segment)\n",
    "    #loop through all the genres\n",
    "    #dirpath: folder we are currently in\n",
    "    #dirnames: all the names of the subfolders\n",
    "    #filenames: all the filenames\n",
    "    #i: the count. It MUST be included\n",
    "    #os.walk returns a generator, that creates a tuple of values: \n",
    "    #(current_path, directories in current_path, files in current_path).\n",
    "    # - each iteration is a different genre\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        #ensure we're not yet at the dataset level\n",
    "        if dirpath is not dataset_path:\n",
    "            #save the semantic label (the mappings etc)\n",
    "            #dirpath_components: the individual folder names that make up the full path\n",
    "            dirpath_components = dirpath.split(\"\\\\\") #genre/blues => [\"genre\",\"blues\"]\n",
    "            semantic_label = dirpath_components[semanticLabelFolderLocation]\n",
    "            #use the parent directory of a sound file as its label\n",
    "            data[\"mapping\"].append(semantic_label)\n",
    "            for f in filenames:\n",
    "                # load audio file : the path of the file is just its directory path plus its name\n",
    "                file_path = os.path.join(dirpath,f)\n",
    "                signal, sr = librosa.load(file_path,sr=sample_rate)\n",
    "                #process segments to extract MFCC and store data\n",
    "                for s in range (0, num_segments):\n",
    "                    start_sample = num_samples_per_segment * s\n",
    "                    finish_sample = start_sample + num_samples_per_segment\n",
    "                    #the mfcc for data points between start_sample and finish_sample\n",
    "                    mfcc = librosa.feature.mfcc(signal[start_sample:finish_sample],sr=sr,n_fft=n_fft,n_mfcc=n_mfcc, hop_length = hop_length)\n",
    "                    mfcc=mfcc.T\n",
    "                    #store mfcc for segement if it has the expected length\n",
    "                    if len(mfcc)==expected_num_mfcc_vectors_per_segment:\n",
    "                        data[\"mfcc\"].append(mfcc.tolist())\n",
    "                        # first iteration was the dataset path\n",
    "                        correctLabel = allLabels[semantic_label]\n",
    "                        data[\"labels\"].append(correctLabel)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d865513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc_data(data,json_path):\n",
    "    #save what we have created as a jason file\n",
    "    with open(json_path,\"w\") as fp:\n",
    "        json.dump(data,fp,indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab4b4b",
   "metadata": {},
   "source": [
    "<h2>Data Preperation</h2>\n",
    "\n",
    "We use the same MFCC data drawn at 16000 samples per second as we generated for the multi-layer perceptron approach used previously (see SRS_classifier.ipynb).\n",
    "\n",
    "Data structure:\n",
    "\n",
    "$x.\\texttt{shape}=(\\text{num samples},\\text{num time intervals}, \\text{num MFCC variables})$\n",
    "\n",
    "$x_i.\\texttt{shape} = (\\text{num time intervals}, \\text{num mfcc variables}))$\n",
    "\n",
    "\n",
    "$y.\\texttt{shape}=(\\text{num labels})$\n",
    "\n",
    "$y_i=\\text{label}$\n",
    "\n",
    "where $\\text{label} \\in \\{1,2,3\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b40098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from the file\n",
    "#split data into training and test\n",
    "\"\"\"\n",
    "load data from a json file with objects \"mfcc\" and \"labels\"\n",
    "@param dataset_path: path to the json file\n",
    "@return np array of mfcc data, np array of target values\n",
    "\"\"\"\n",
    "def load_data(dataset_path):\n",
    "    with open(dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        inputs=np.array(data[\"mfcc\"])\n",
    "        targets=np.array(data[\"labels\"])\n",
    "        return inputs,targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac237e91",
   "metadata": {},
   "source": [
    "<h3>Load the data</h3>\n",
    "Load the data from the jason file and verify that the inputs are of the expected type, and the sample size is as expected, in this case: 480 samples of each label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f6013dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape (1440, 94, 13)\n",
      "Frequency of unique values of the targets:\n",
      "[[  1   2   3]\n",
      " [480 480 480]]\n"
     ]
    }
   ],
   "source": [
    "x,y = load_data(\"data.json\")\n",
    "print (\"input shape\", x.shape)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the targets:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f14c8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a05e098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1166, 94, 13)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d1f7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "Creates a model of three pooled convolutional layers, followed by a single fully connected layer and an output layer.\n",
    "@param input_shape: the structure of SINGLE value x_i (omits the first dimension of x which is the total number of samples.)\n",
    "@return uncompiled keras model of the CNN.\n",
    "\"\"\"\n",
    "def build_model(input_shape,dropoutRate=0):\n",
    "    model=keras.Sequential()\n",
    "    \n",
    "    #LSTM Layer 1\n",
    "    #return_sequences = True: sequence to sequence layer, otherwise sequence to vector\n",
    "    model.add(keras.layers.LSTM(units=64,activation='tanh',input_shape=input_shape,return_sequences=True))\n",
    "    #LSTM Layer 2\n",
    "    model.add(keras.layers.LSTM(units=64,activation='tanh',input_shape=input_shape,return_sequences=False))\n",
    "    \n",
    "    #pipe output into a dense layer. No need to flatten because sequence to vector\n",
    "    model.add(keras.layers.Dense(units=64,activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropoutRate))\n",
    "    \n",
    "    #output layer\n",
    "    #a fully connected layer for classification\n",
    "    NUMBEROFPOSSIBLEOUTPUTS=4\n",
    "    model.add(keras.layers.Dense(NUMBEROFPOSSIBLEOUTPUTS,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e1778f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each sample has the shape (num samples,intervals,variables,channels=1)\n",
    "input_shape=(x_train.shape[1], x_train.shape[2])\n",
    "model = build_model(input_shape,dropoutRate=0.4)\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "462cc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam is a very very effecting sgd variant for deep learning\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#put all the components together\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eac9e943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 2s 46ms/step - loss: 1.2140 - accuracy: 0.4674 - val_loss: 1.1479 - val_accuracy: 0.4846\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 1.0900 - accuracy: 0.5266 - val_loss: 1.0378 - val_accuracy: 0.5769\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.9769 - accuracy: 0.6046 - val_loss: 0.9249 - val_accuracy: 0.6385\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.8582 - accuracy: 0.6655 - val_loss: 0.7888 - val_accuracy: 0.6615\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.7043 - accuracy: 0.7384 - val_loss: 0.6311 - val_accuracy: 0.7231\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.5728 - accuracy: 0.7719 - val_loss: 0.5319 - val_accuracy: 0.7615\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.5184 - accuracy: 0.7907 - val_loss: 0.5230 - val_accuracy: 0.7615\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.4778 - accuracy: 0.8079 - val_loss: 0.4882 - val_accuracy: 0.7846\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.4517 - accuracy: 0.8216 - val_loss: 0.4241 - val_accuracy: 0.8000\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.3820 - accuracy: 0.8559 - val_loss: 0.4241 - val_accuracy: 0.8308\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.3408 - accuracy: 0.8782 - val_loss: 0.3611 - val_accuracy: 0.8462\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.3212 - accuracy: 0.8825 - val_loss: 0.3316 - val_accuracy: 0.8923\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.2970 - accuracy: 0.8979 - val_loss: 0.3347 - val_accuracy: 0.8846\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.2642 - accuracy: 0.9005 - val_loss: 0.3700 - val_accuracy: 0.8692\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.2637 - accuracy: 0.9117 - val_loss: 0.3139 - val_accuracy: 0.9000\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.2220 - accuracy: 0.9331 - val_loss: 0.3291 - val_accuracy: 0.8692\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.2230 - accuracy: 0.9357 - val_loss: 0.2673 - val_accuracy: 0.9000\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1963 - accuracy: 0.9391 - val_loss: 0.2730 - val_accuracy: 0.9077\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1823 - accuracy: 0.9434 - val_loss: 0.2471 - val_accuracy: 0.9154\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1603 - accuracy: 0.9511 - val_loss: 0.2347 - val_accuracy: 0.9231\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1550 - accuracy: 0.9605 - val_loss: 0.2226 - val_accuracy: 0.9462\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1498 - accuracy: 0.9666 - val_loss: 0.2349 - val_accuracy: 0.9231\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1455 - accuracy: 0.9631 - val_loss: 0.2101 - val_accuracy: 0.9308\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1558 - accuracy: 0.9614 - val_loss: 0.2201 - val_accuracy: 0.9308\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1288 - accuracy: 0.9666 - val_loss: 0.1918 - val_accuracy: 0.9385\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1158 - accuracy: 0.9700 - val_loss: 0.1411 - val_accuracy: 0.9538\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1061 - accuracy: 0.9743 - val_loss: 0.1873 - val_accuracy: 0.9308\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0950 - accuracy: 0.9803 - val_loss: 0.1761 - val_accuracy: 0.9462\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0966 - accuracy: 0.9803 - val_loss: 0.1771 - val_accuracy: 0.9462\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0917 - accuracy: 0.9811 - val_loss: 0.1575 - val_accuracy: 0.9538\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0913 - accuracy: 0.9786 - val_loss: 0.1939 - val_accuracy: 0.9462\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0915 - accuracy: 0.9794 - val_loss: 0.1931 - val_accuracy: 0.9385\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0835 - accuracy: 0.9794 - val_loss: 0.2332 - val_accuracy: 0.9385\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0961 - accuracy: 0.9751 - val_loss: 0.1822 - val_accuracy: 0.9538\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1551 - accuracy: 0.9477 - val_loss: 0.1990 - val_accuracy: 0.9538\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0883 - accuracy: 0.9828 - val_loss: 0.1561 - val_accuracy: 0.9385\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0821 - accuracy: 0.9811 - val_loss: 0.1407 - val_accuracy: 0.9615\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0908 - accuracy: 0.9803 - val_loss: 0.1505 - val_accuracy: 0.9615\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0682 - accuracy: 0.9846 - val_loss: 0.1665 - val_accuracy: 0.9462\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0696 - accuracy: 0.9837 - val_loss: 0.1932 - val_accuracy: 0.9538\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0858 - accuracy: 0.9786 - val_loss: 0.1523 - val_accuracy: 0.9615\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0731 - accuracy: 0.9846 - val_loss: 0.2144 - val_accuracy: 0.9385\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0618 - accuracy: 0.9863 - val_loss: 0.1810 - val_accuracy: 0.9538\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0568 - accuracy: 0.9880 - val_loss: 0.2385 - val_accuracy: 0.9385\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0499 - accuracy: 0.9863 - val_loss: 0.1943 - val_accuracy: 0.9462\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0512 - accuracy: 0.9889 - val_loss: 0.2249 - val_accuracy: 0.9462\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0502 - accuracy: 0.9854 - val_loss: 0.2213 - val_accuracy: 0.9308\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0456 - accuracy: 0.9897 - val_loss: 0.2252 - val_accuracy: 0.9385\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0901 - accuracy: 0.9760 - val_loss: 0.1712 - val_accuracy: 0.9615\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0545 - accuracy: 0.9863 - val_loss: 0.1981 - val_accuracy: 0.9615\n"
     ]
    }
   ],
   "source": [
    "#stores the progression of several metrics as the model trains.\n",
    "history = model.fit(x_train,y_train,validation_data=(x_validation,y_validation),epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0073d2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5c268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c667c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d9589cfbf114>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f9a86-b3b2-4040-bc42-be5169f828a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "model = Sequential()\n",
    "model.add(layers.LSTM(256, input_shape=(1, 66), return_sequences=True))\n",
    "model.summary()\n",
    "Model: \"sequential\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce5304-acbf-4db0-a90c-b0e9be71768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5bf110",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacd86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
